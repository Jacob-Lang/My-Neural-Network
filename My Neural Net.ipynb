{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network from scratch\n",
    "\n",
    "In this excercise I want to build a neural network from scratch to make sure I understand all the elements properly. \n",
    "It won't be particularly good but it should at least work. \n",
    "I will test it on the MNIST data set. \n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jacobnet as jn\n",
    "#from jacobnet import layer\n",
    "#from keras.datasets import mnist\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# I use keras only to handle the MNIST data set. Not to make the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Layer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jn.layer.Layer().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x)/(1 + np.exp(-x))**2\n",
    "sigmoid_prime = np.vectorize(sigmoid_prime)\n",
    "\n",
    "\n",
    "def loss(output, label_vec):\n",
    "    L = 1/10*mp.linalg.norm(output - label_vec)**2\n",
    "    return L\n",
    "\n",
    "def loss_derivative(output):\n",
    "    return 2/10*output.T\n",
    "\n",
    "def label_to_vec(number):\n",
    "    v = np.zeros((10,1))\n",
    "    v[number] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise weights and biases of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size is 28x28=784 \n",
    "# hidden layer size 50\n",
    "# output layer size 10\n",
    "W = [0, 0]\n",
    "b = [0, 0]\n",
    "\n",
    "# hidden layer\n",
    "W[0] = np.random.rand(50, 784)/(784*50)\n",
    "b[0] = np.zeros((50, 1))\n",
    "\n",
    "# output layer\n",
    "W[1] = np.random.rand(10, 50)/(50*10)\n",
    "b[1] = np.zeros((10, 1))\n",
    "\n",
    "def predict(v):\n",
    "    v = sigmoid(np.dot(W[0], v) + b[0])\n",
    "    v = sigmoid(np.dot(W[1], v) + b[1])\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.num_layers = 2\n",
    "        self.input_size = 784\n",
    "        self.nodes_per_layer = [50, 10]\n",
    "        self.W = [np.random.rand(50, 784)/(784*50), np.random.rand(10, 50)/(50*10)]\n",
    "        self.b = [np.zeros((50, 1)), np.zeros((10, 1))]\n",
    "        self.eta = 0.1\n",
    "        \n",
    "    def forward(self, input_vec):\n",
    "        v = input_vec\n",
    "        for n_layer in range(self.num_layers):\n",
    "            v = sigmoid(np.dot(self.W[n_layer], v) + self.b[n_layer])\n",
    "        output_vec = v\n",
    "        return output_vec\n",
    "        \n",
    "    def backpropagate(self, input_vec, label):\n",
    "        # includes a forward pass to record the outputs of each layer. \n",
    "        v = np.ndarray.flatten(input_vec/255).reshape(784,1)\n",
    "        z_store = [0]  # z_0 doesn't exist so call it 0\n",
    "        a_store = [v]  # a_0 is input\n",
    "        dW = self.W # these will be updated.\n",
    "        db = self.b\n",
    "        for n_layer in range(self.num_layers):\n",
    "            v = np.dot(self.W[n_layer], v) + self.b[n_layer]\n",
    "            z_store.append(v)\n",
    "            v = sigmoid(v)\n",
    "            a_store.append(v)\n",
    "            \n",
    "        # now pass backward to calculate derivative. \n",
    "        da = 2/10*(a_store[-1] - label_to_vec(label)).T\n",
    "        dz = np.dot(da, np.diag(sigmoid_prime(z_store[-1]).T[0]))\n",
    "        db[-1] = dz\n",
    "        #dW[-1] = np.dot(dz, )\n",
    "        # update last layer\n",
    "        #self.b[-1] += -self.eta*db[-1]**2\n",
    "        #self.W[-1] += -self.eta*dW[-1]**2\n",
    "        \n",
    "        # pass back derivative\n",
    "        da = np.dot(dz, self.W[-1])\n",
    "        dz = np.dot(da, np.diag(sigmoid_prime(z_store[-2]).T[0]))\n",
    "        db[-2] = dz\n",
    "       # dW[-2] = np.dot(dz, )\n",
    "        # update\n",
    "        #self.b[-2] += -self.eta*db[-2]**2\n",
    "        #self.W[-2] += -self.eta*dW[-2]**2\n",
    "        \n",
    "        return db, dW\n",
    "    \n",
    "    def batch_train(self, x_train, y_train):\n",
    "        \n",
    "        N_batch = len(y_train)\n",
    "        \n",
    "        db = [0 for n in range(N_batch)]\n",
    "        dW = [0 for n in range(N_batch)]\n",
    "        \n",
    "        for n in range(N_batch):\n",
    "            X = x_train[n]\n",
    "            Y = y_train[n]\n",
    "            db[n], dW[n] = self.backpropagate(X, Y)\n",
    "    \n",
    "        db = sum(db)/N_batch\n",
    "        dW = sum(dW)/N_batch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,50) (1,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-1ac4e7c7cac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#nn.backpropagate(x_train[0],y_train[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-190-d66ecafd125a>\u001b[0m in \u001b[0;36mbatch_train\u001b[1;34m(self, x_train, y_train)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-d66ecafd125a>\u001b[0m in \u001b[0;36mbackpropagate\u001b[1;34m(self, input_vec, label)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mz_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,50) (1,10) "
     ]
    }
   ],
   "source": [
    "nn = MyNeuralNetwork()\n",
    "#nn.backpropagate(x_train[0],y_train[0])\n",
    "nn.batch_train(x_train[:10],y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, dW = nn.backpropagate(np.ndarray.flatten(x_train[0]/255).reshape(784,1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_vec(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
